{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qwRX_2RCrWdg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SPlwg9AgrYgp",
    "outputId": "d2b853ad-f08f-40fa-e9a7-8dfa4c432469"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 users\n",
      "1682 items\n"
     ]
    }
   ],
   "source": [
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('ml-100k/u.data', sep='\\t', names=names)\n",
    "df.head()\n",
    "\n",
    "i_cols = ['movie id', 'movie title' ,'release date','video release date', 'IMDb URL', 'unknown', 'Action', 'Adventure',\n",
    " 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    " 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "items = pd.read_csv('ml-100k/u.item', sep='|', names=i_cols, encoding='latin-1')\n",
    "\n",
    "items.head()\n",
    "\n",
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "print(str(n_users) + ' users')\n",
    "print(str(n_items) + ' items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "name the features\n",
    "check the number of users and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KlU-Lq16rlpN",
    "outputId": "652c555a-4e2e-478c-ebd5-bfb0dcb65eac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       user_id  item_id  rating  timestamp\n",
       " 84196      629      632       3  880117031\n",
       " 87893      940       98       4  885921421\n",
       " 46671      639      796       1  891240805\n",
       " 67505      744      340       3  881171820\n",
       " 31329      328       44       3  885047864\n",
       " ...        ...      ...     ...        ...\n",
       " 52239      627      810       3  879531459\n",
       " 55169      710      335       1  882063564\n",
       " 70002      458      762       3  886395065\n",
       " 87627      344      117       3  884899767\n",
       " 55861      752     1105       3  891207983\n",
       " \n",
       " [80000 rows x 4 columns],\n",
       "        user_id  item_id  rating  timestamp\n",
       " 66807      843      447       2  879443297\n",
       " 84389      907     1284       5  881030348\n",
       " 83678      715       31       4  875963692\n",
       " 97427      759      237       3  881476891\n",
       " 49817      681      259       2  885409882\n",
       " ...        ...      ...     ...        ...\n",
       " 56790      413       14       5  879969513\n",
       " 64497      395      186       5  883764817\n",
       " 78371      144     1028       3  888104495\n",
       " 74760      783      307       5  884326506\n",
       " 75424      239      304       1  889181248\n",
       " \n",
       " [20000 rows x 4 columns])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the data into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvaLspXUrg9L",
    "outputId": "fab9c6db-5d87-455a-a94f-ba8621f72d31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     0     1     2     3     4     5     6     7     8     9     ...  1672  \\\n",
       " 0     5.0   3.0   4.0   0.0   3.0   5.0   0.0   1.0   0.0   3.0  ...   0.0   \n",
       " 1     4.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   2.0  ...   0.0   \n",
       " 2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       " 3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       " 4     4.0   3.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       " ..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       " 938   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   5.0   0.0  ...   0.0   \n",
       " 939   0.0   0.0   0.0   2.0   0.0   0.0   4.0   0.0   3.0   0.0  ...   0.0   \n",
       " 940   5.0   0.0   0.0   0.0   0.0   0.0   4.0   0.0   0.0   0.0  ...   0.0   \n",
       " 941   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       " 942   0.0   5.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       " \n",
       "      1673  1674  1675  1676  1677  1678  1679  1680  1681  \n",
       " 0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 1     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " ..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       " 938   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 939   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 940   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 941   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 942   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " \n",
       " [943 rows x 1682 columns],\n",
       "      0     1     2     3     4     5     6     7     8     9     ...  1672  \\\n",
       " 0     0.0   0.0   0.0   3.0   0.0   0.0   4.0   0.0   5.0   0.0  ...   0.0   \n",
       " 1     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       " 2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       " 3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       " 4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       " ..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       " 938   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       " 939   0.0   0.0   0.0   0.0   0.0   0.0   0.0   5.0   0.0   0.0  ...   0.0   \n",
       " 940   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       " 941   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       " 942   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   3.0   0.0  ...   0.0   \n",
       " \n",
       "      1673  1674  1675  1676  1677  1678  1679  1680  1681  \n",
       " 0     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 1     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 2     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 3     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 4     0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " ..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       " 938   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 939   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 940   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 941   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " 942   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       " \n",
       " [943 rows x 1682 columns])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = np.zeros((n_users, n_items))\n",
    "for row in train_df.itertuples():\n",
    "    train_ds[row[1]-1, row[2]-1] = row[3]\n",
    "train_ds = pd.DataFrame(train_ds)\n",
    "\n",
    "test_ds = np.zeros((n_users, n_items))\n",
    "for row in test_df.itertuples():\n",
    "    test_ds[row[1]-1, row[2]-1] = row[3]\n",
    "test_ds = pd.DataFrame(test_ds)\n",
    "\n",
    "train_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a user-item table with the corresponding ratings for training data set\n",
    "Create a user-item table with the corresponding ratings for test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KgPMV2sobQZW",
    "outputId": "97f5fec9-ce61-4df0-c7c4-050485aa78d3"
   },
   "outputs": [],
   "source": [
    "DELTA = 25\n",
    "EPSILON = 1e-9\n",
    "\n",
    "np_item_pearson_corr = np.zeros((n_items, n_items))\n",
    "\n",
    "#for every item (T is the transpose of train_ds and values)to make rows items.\n",
    "for i, item_i_vec in enumerate(train_ds.T.values):\n",
    "    #for every item.\n",
    "    for j, item_j_vec in enumerate(train_ds.T.values):\n",
    "\n",
    "        mask_i = item_i_vec > 0\n",
    "        mask_j = item_j_vec > 0\n",
    "\n",
    "        #np.where(mask_i) will return indexes of the columns where there are values (the users that have rated i)\n",
    "        #np.where(mask_j) will return indexes of the columns where there are values (the users that have rated j)\n",
    "        #np.intersect will return the indexes of common users, that have rated both i and j.\n",
    "        corrated_index = np.intersect1d(np.where(mask_i), np.where(mask_j))\n",
    "        \n",
    "        #if there are no common ratings, do nothing.\n",
    "        if len(corrated_index) == 0:\n",
    "            continue\n",
    "        \n",
    "        #the numerator is the sum of the ratings for i and j\n",
    "        #np.clip will remap values smaller than 0 to 0 and values larger than 1, to 1, for all ratings of item i and j.\n",
    "            #as the minimum rating is 1, this will provide a count for how many total ratings there are.\n",
    "        #np.sum will sum how many ratings there are for i and j.\n",
    "        #EPSILON will make sure that there is no null divisor.\n",
    "        #The result is the mean rating for i and j.\n",
    "        mean_item_i = np.sum(item_i_vec) / (np.sum(np.clip(item_i_vec, 0, 1)) + EPSILON)\n",
    "        mean_item_j = np.sum(item_j_vec) / (np.sum(np.clip(item_j_vec, 0, 1)) + EPSILON)\n",
    "\n",
    "        #take the mean ratings of i and j from each rating for i and j\n",
    "        item_i_sub_mean = item_i_vec[corrated_index] - mean_item_i\n",
    "        item_j_sub_mean = item_j_vec[corrated_index] - mean_item_j\n",
    "        \n",
    "        #square the ratings for i and j\n",
    "        r_ui_sub_ri_sq = np.square(item_i_sub_mean)\n",
    "        r_uj_sub_rj_sq = np.square(item_j_sub_mean)\n",
    "\n",
    "        #root the sum of the squares of the ratings for i and j.\n",
    "        r_ui_sub_ri_sq_sum_sqrt = np.sqrt(np.sum(r_ui_sub_ri_sq))\n",
    "        r_uj_sub_rj_sq_sum_sqrt = np.sqrt(np.sum(r_uj_sub_rj_sq))\n",
    "\n",
    "        #numerator\n",
    "            #multiply all of the normalised ratings for i and j\n",
    "            #sum the result\n",
    "        #denominator\n",
    "            #Multiply the root of the sum of the squares for i and j\n",
    "            #Add epsilon to stop divisions by 0\n",
    "        #This yields the centered cosine similarity between i and j\n",
    "        sim = np.sum(item_i_sub_mean * item_j_sub_mean) / (r_ui_sub_ri_sq_sum_sqrt * r_uj_sub_rj_sq_sum_sqrt + EPSILON)\n",
    "\n",
    "        #If the size of the co-rated item set is too small, the corresponding similarity is likely not that reliable.\n",
    "        #To weight it, we take in to account how many items are in the co-rated set.\n",
    "        #If there are less than 25 (DELTA) items, it will get less weight.\n",
    "        #If there are more than 25 items, the weight will stay the same\n",
    "        weighted_sim = (min(len(corrated_index), DELTA) / DELTA) * sim\n",
    "\n",
    "        np_item_pearson_corr[i][j] = weighted_sim\n",
    "\n",
    "#This now holds the correlations between each item.\n",
    "np_item_pearson_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyysZ6e7bQZX"
   },
   "outputs": [],
   "source": [
    "#create a matrix to store the predictions for each user-item pair\n",
    "np_predictions = np.zeros((n_users, n_items))\n",
    "\n",
    "K = 10\n",
    "EPSILON = 1e-9\n",
    "\n",
    "#for each user-item pair in the test dataset\n",
    "for (user, item), rating in np.ndenumerate(test_ds.values):\n",
    "    if rating > 0:\n",
    "        #if there is an existing rating, \n",
    "        #What is being sorted:\n",
    "            #The ids of similarities between item and the other items\n",
    "        #Result of the sorted:\n",
    "            #Ids of similarity between item and other items , sorted ascendingly by similarity value.\n",
    "        #Sim_item_ids:\n",
    "            #The ids of the top-ten most similar items with the current item.\n",
    "        sim_item_ids = np.argsort(np_item_pearson_corr[item])[-(K + 1):-1]\n",
    "        \n",
    "        #The similarities of the top ten most similar items to the current item.\n",
    "        sim_val = np_item_pearson_corr[item][sim_item_ids]\n",
    "    \n",
    "        #sim_items is the item vectors of the top ten most similar items to the current item\n",
    "        sim_items = train_ds.T.values[sim_item_ids]\n",
    "        \n",
    "        #Numerator\n",
    "            #The sum of the ratings of the current item\n",
    "        #Denominator\n",
    "            #The count of ratings of the current item\n",
    "        #Result\n",
    "            #The average rating of the current item\n",
    "        item_mean = np.sum(train_ds.T.values[item]) / (np.sum(np.clip(train_ds.T.values[item], 0, 1)) + EPSILON)\n",
    "        \n",
    "        #Numerator\n",
    "            #The sum of the ratings for each item\n",
    "        #Denominator\n",
    "            #The count of the ratings for each item\n",
    "        #Result\n",
    "            #The average rating of each item\n",
    "        sim_item_mean = np.sum(sim_items, axis=1) / (np.sum(np.clip(sim_items, 0, 1), axis=1) + EPSILON)\n",
    "\n",
    "        #What is being multiplied\n",
    "            #The similarities of the top ten most similar items to the current item.\n",
    "            #The ratings of the user for the most similar items, subtracted by the average rating for that item.\n",
    "        #Result\n",
    "            #The weighted ratings of the current user to the top ten most similar items, weighted by similarity. (Centering)\n",
    "        sim_r_sum_mean = sim_val * (sim_items[:, user] - sim_item_mean) \n",
    "\n",
    "        #w is the boolean of whether the current user rated the 10 most similar items\n",
    "        w = np.clip(sim_items[:, user], 0, 1)\n",
    "        \n",
    "        #Only the weighted ratings that are actually items rated by user are kept.\n",
    "        sim_r_sum_mean *= w\n",
    "\n",
    "        #The prediction is\n",
    "            #The average rating for the current item (calculated with all users)\n",
    "            # +\n",
    "            #Numerator\n",
    "                #The sum of the weighted ratings that the user has rated, of the top ten most similar items\n",
    "            #Denominator \n",
    "                #The sum of the similarities of the items that the user has rated, of the top ten most similar items.\n",
    "            #Result\n",
    "                # The weighted rating \n",
    "        np_predictions[user][item] = item_mean + np.sum(sim_r_sum_mean) / (np.sum(sim_val * w) + EPSILON)  \n",
    "        \n",
    "        #The prediction is clipped at 0 and 5.\n",
    "            #Should this not be 1 and 5? It can be, but does not have to be.\n",
    "        np_predictions[user][item] = np.clip(np_predictions[user][item], 0, 5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bg2OzOBXbQZZ",
    "outputId": "8e032639-a4ca-4d72-8194-7c2a36611e94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE on Tesing set (Item-based): 0.8129100318152723\n"
     ]
    }
   ],
   "source": [
    "labels = test_ds.values\n",
    "\n",
    "absolute_error = np.abs(np_predictions - labels)\n",
    "\n",
    "weight = np.clip(labels, 0, 1)\n",
    "\n",
    "abs_error = absolute_error * weight\n",
    "\n",
    "MAE = np.sum(abs_error) / np.sum(weight)\n",
    "\n",
    "print(\"MAE on Tesing set (Item-based): \" + str(MAE));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEh_WNPXbQZa",
    "outputId": "3be0306d-72e2-4a82-fc84-06f00b746193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on Tesing set (Item-based): 1.0504356631295801\n"
     ]
    }
   ],
   "source": [
    "labels = test_ds.values\n",
    "\n",
    "squared_error = np.square(np_predictions - labels)\n",
    "weight = np.clip(labels, 0, 1)\n",
    "\n",
    "squared_error = squared_error * weight\n",
    "\n",
    "RMSE = np.sqrt(np.sum(squared_error) / np.sum(weight))\n",
    "\n",
    "print(\"RMSE on Tesing set (Item-based): \" + str(RMSE));"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KNN_based_CF_final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
